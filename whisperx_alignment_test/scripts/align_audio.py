#!/usr/bin/env python3
"""WhisperX å¼·åˆ¶å°é½Šä¸»è…³æœ¬

æ­¤æ¨¡çµ„å¯è¢«ç›´æ¥åŸ·è¡Œï¼Œä¹Ÿå¯ç”±å…¶ä»–å·¥å…·åŒ¯å…¥ã€‚
CLI æ”¯æ´è‡ªè¨‚éŸ³é »ã€è…³æœ¬ã€è¼¸å‡ºè³‡æ–™å¤¾ä»¥åŠè£ç½® (CPU/MPS/CUDA)ã€‚
ç•¶æä¾›è…³æœ¬æ™‚æœƒå…ˆç¶“é `text_cleaner` è™•ç†ï¼Œä¸¦ä»¥æ¸…ç†å¾Œçš„æ®µè½
è¦†è“‹ Whisper è‡ªå‹•è½‰éŒ„çš„çµæœï¼Œé”æˆ Forced Alignmentã€‚
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from text_cleaner import clean_script_for_alignment, prepare_segments


def check_whisperx_installed():
    """æª¢æŸ¥ WhisperX æ˜¯å¦å·²å®‰è£"""
    try:
        import whisperx
        return True
    except ImportError:
        return False


def resolve_device(device: str) -> str:
    """å°‡ä½¿ç”¨è€…è¼¸å…¥çš„ device è½‰æ›ç‚ºå¯ä»¥è¢« whisperx æ¥å—çš„å€¼ã€‚"""

    if device.lower() != "auto":
        return device.lower()

    try:
        import torch

        if torch.cuda.is_available():
            return "cuda"
        if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
            return "mps"
    except Exception:
        pass

    return "cpu"


def align_with_whisperx(
    audio_path: Path,
    device: str = "cpu",
    language: str = "en",
    reference_text: Optional[str] = None,
    segments_override: Optional[List[Dict[str, Any]]] = None,
    vad_method: str = "silero",
    whisper_model: str = "small",
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ WhisperX å®Œæ•´æµç¨‹ï¼šè½‰éŒ„ + å°é½Š

    Args:
        audio_path: éŸ³é »æ–‡ä»¶è·¯å¾‘
        device: è¨­å‚™é¡å‹ ("cpu", "mps", "cuda")
        language: èªè¨€ä»£ç¢¼

    Returns:
        å°é½Šçµæœ
    """
    import whisperx

    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
    print(f"ğŸŒ èªè¨€: {language}")

    # 1. è¼‰å…¥éŸ³é »
    print("\nğŸ“‚ è¼‰å…¥éŸ³é »...")
    audio = whisperx.load_audio(str(audio_path))
    print(f"âœ… éŸ³é »è¼‰å…¥æˆåŠŸ: {len(audio) / 16000:.1f} ç§’")

    # 2. è¼‰å…¥ Whisper æ¨¡å‹ä¸¦è½‰éŒ„
    whisper_model_name = whisper_model
    print(f"\nğŸ™ï¸ è¼‰å…¥ Whisper æ¨¡å‹ ({whisper_model_name})...")
    compute_type = "int8"
    if device in {"mps", "cuda"}:
        compute_type = "float16"
    model = whisperx.load_model(
        whisper_model_name,
        device,
        compute_type=compute_type,
        language=language,
        vad_method=vad_method,
    )
    print(f"âœ… Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ (compute_type: {compute_type})")

    print("\nğŸ“ åŸ·è¡Œè½‰éŒ„...")
    start_time = datetime.now()

    # WhisperX FasterWhisperPipeline æ”¯æŒçš„åƒæ•¸ï¼š
    # audio, batch_size, num_workers, language, task, chunk_size, print_progress, combined_progress, verbose
    result = model.transcribe(
        audio,
        batch_size=16,
        language=language
    )
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"âœ… è½‰éŒ„å®Œæˆï¼è€—æ™‚: {elapsed:.1f} ç§’")
    print(f"ğŸ“Š åµæ¸¬èªè¨€: {result.get('language', 'unknown')}")
    print(f"ğŸ“Š è½‰éŒ„æ®µè½: {len(result.get('segments', []))}")

    # 3. è¼‰å…¥å°é½Šæ¨¡å‹ä¸¦åŸ·è¡Œå¼·åˆ¶å°é½Š
    print(f"\nğŸ” è¼‰å…¥ {language} å°é½Šæ¨¡å‹...")
    try:
        align_model, metadata = whisperx.load_align_model(
            language_code=result.get("language", language),
            device=device
        )
        print("âœ… å°é½Šæ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å°é½Šæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        raise

    print("\nâš¡ åŸ·è¡Œå¼·åˆ¶å°é½Šï¼ˆè©ç´šæ™‚é–“æˆ³ï¼‰...")
    start_time = datetime.now()

    try:
        # âœ… ä½¿ç”¨ WhisperX æœ€ä½³å¯¦è¸ï¼šç›´æ¥ä½¿ç”¨ Whisper çš„è½‰éŒ„æ®µè½
        # Whisper æä¾›åŸºæ–¼çœŸå¯¦éŸ³é »çš„ç²—ç•¥æ™‚é–“æˆ³ï¼Œå¼·åˆ¶å°é½Šæœƒå°‡å…¶ç²¾ä¿®åˆ°è©ç´š
        segments_to_align = result["segments"]

        if segments_override:
            # ä¿ç•™åƒè€ƒè…³æœ¬ä¿¡æ¯ç”¨æ–¼æ—¥èªŒï¼Œä½†ä¸æ›¿æ›æ™‚é–“æˆ³
            print(f"â„¹ï¸  åƒè€ƒè…³æœ¬åŒ…å« {len(segments_override)} æ®µï¼ˆåƒ…ç”¨æ–¼é©—è­‰ï¼Œä¸æ›¿æ›æ™‚é–“æˆ³ï¼‰")
            print(f"â„¹ï¸  ä½¿ç”¨ Whisper è½‰éŒ„çš„ {len(segments_to_align)} å€‹æ®µè½é€²è¡Œå°é½Š")

        result = whisperx.align(
            segments_to_align,
            align_model,
            metadata,
            audio,
            device,
            return_char_alignments=False
        )

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… å°é½Šå®Œæˆï¼è€—æ™‚: {elapsed:.1f} ç§’")

        # çµ±è¨ˆè³‡è¨Š
        total_words = sum(len(seg.get('words', [])) for seg in result.get('segments', []))
        print(f"ğŸ“Š å°é½Šè©æ•¸: {total_words}")

        if reference_text:
            result["reference_text"] = reference_text

        return result

    except Exception as e:
        print(f"âŒ å°é½Šå¤±æ•—: {e}")
        raise


def save_alignment_json(result: Dict[str, Any], output_path: Path):
    """ä¿å­˜å°é½Šçµæœç‚º JSON"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON å·²ä¿å­˜: {output_path}")


def generate_srt(result: Dict[str, Any], output_path: Path):
    """ç”Ÿæˆ SRT å­—å¹•æ–‡ä»¶"""

    def format_timestamp(seconds: float) -> str:
        """è½‰æ›ç§’æ•¸ç‚º SRT æ™‚é–“æ ¼å¼ (HH:MM:SS,mmm)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    with open(output_path, 'w', encoding='utf-8') as f:
        subtitle_index = 1

        for segment in result.get('segments', []):
            for word_info in segment.get('words', []):
                word = word_info.get('word', '').strip()
                start = word_info.get('start', 0.0)
                end = word_info.get('end', 0.0)

                if word:  # åªè¼¸å‡ºæœ‰å…§å®¹çš„è©
                    f.write(f"{subtitle_index}\n")
                    f.write(f"{format_timestamp(start)} --> {format_timestamp(end)}\n")
                    f.write(f"{word}\n\n")
                    subtitle_index += 1

    print(f"âœ… SRT å·²ä¿å­˜: {output_path}")


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="WhisperX å¼·åˆ¶å°é½Šå·¥å…·")
    parser.add_argument("--audio", type=Path, required=True, help="éŸ³é »æª”æ¡ˆ (wav/mp3)")
    parser.add_argument("--script", type=Path, help="å°é½Šç”¨çš„è…³æœ¬æª” (å¯é¸)")
    parser.add_argument("--out-dir", type=Path, default=Path("output"), help="è¼¸å‡ºè³‡æ–™å¤¾")
    parser.add_argument("--device", default="auto", help="è£ç½®ï¼šauto/cpu/mps/cuda")
    parser.add_argument("--language", default="en", help="èªè¨€ä»£ç¢¼ (é è¨­ en)")
    parser.add_argument("--max-words", type=int, default=50, help="è…³æœ¬åˆ†æ®µæœ€å¤§è©æ•¸")
    parser.add_argument("--force", action="store_true", help="è‹¥è¼¸å‡ºå­˜åœ¨å‰‡è¦†å¯«")
    parser.add_argument(
        "--vad-method",
        default="silero",
        choices=["silero", "pyannote"],
        help="é¸æ“‡èªéŸ³æ´»å‹•åµæ¸¬ (VAD) æ¨¡å‹ï¼Œé è¨­ä½¿ç”¨ sileroï¼ˆè¼ƒæ–°ä¸”ç„¡éœ€ pyannoteï¼‰",
    )
    parser.add_argument(
        "--model",
        default="small",
        choices=["tiny", "base", "small", "medium", "large-v2", "large-v3"],
        help="Whisper æ¨¡å‹å°ºå¯¸ï¼Œé è¨­ small",
    )

    args = parser.parse_args(argv)

    print("=" * 60)
    print("ğŸ¯ WhisperX å¼·åˆ¶å°é½Š")
    print("=" * 60)

    if not check_whisperx_installed():
        print("\nâŒ WhisperX æœªå®‰è£ï¼è«‹å…ˆåŸ·è¡Œ `pip install whisperx`");
        return 1

    audio_file: Path = args.audio
    script_file: Optional[Path] = args.script
    output_dir: Path = args.out_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    if not audio_file.exists():
        print(f"âŒ éŸ³é »æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        return 1
    if script_file and not script_file.exists():
        print(f"âŒ è…³æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_file}")
        return 1

    json_output = output_dir / "aligned_transcript.json"
    srt_output = output_dir / "subtitles.srt"
    if (json_output.exists() or srt_output.exists()) and not args.force:
        print("âš ï¸  è¼¸å‡ºæª”å·²å­˜åœ¨ï¼Œè‹¥è¦è¦†å¯«è«‹åŠ ä¸Š --force")
        print(f"  JSON: {json_output}")
        print(f"  SRT : {srt_output}")
        return 0

    device = resolve_device(args.device)
    print(f"ğŸ“‚ éŸ³é »: {audio_file}")
    if script_file:
        print(f"ğŸ“ åƒè€ƒè…³æœ¬: {script_file}")
    print(f"âš™ï¸  ä½¿ç”¨è£ç½®: {device}")

    segments_override: Optional[List[Dict[str, Any]]] = None
    clean_reference: Optional[str] = None
    if script_file:
        raw_text = script_file.read_text(encoding="utf-8")
        clean_reference = clean_script_for_alignment(raw_text)
        segments_override = prepare_segments(raw_text, max_words_per_segment=args.max_words)

    try:
        result = align_with_whisperx(
            audio_path=audio_file,
            device=device,
            language=args.language,
            reference_text=clean_reference,
            segments_override=segments_override,
            vad_method=args.vad_method,
            whisper_model=args.model,
        )

        save_alignment_json(result, json_output)
        generate_srt(result, srt_output)

        print("\n" + "=" * 60)
        print("ğŸ‰ å°é½Šå®Œæˆï¼")
        print("=" * 60)
        print(f"  ğŸ“„ JSON: {json_output}")
        print(f"  ğŸ“„ SRT : {srt_output}")
        return 0

    except Exception as exc:
        print(f"\nâŒ éŒ¯èª¤: {exc}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
