#!/usr/bin/env python3
"""
æ–‡æœ¬æ¸…ç†å·¥å…· - ç‚º WhisperX å¼·åˆ¶å°é½Šæº–å‚™è…³æœ¬
ç§»é™¤æš«åœæ¨™è¨˜ã€ä¸­æ–‡è¨»é‡‹ç­‰éèªéŸ³å…§å®¹
"""

import re
from typing import List, Dict


def clean_script_for_alignment(script_text: str) -> str:
    """
    æ¸…ç†è…³æœ¬ï¼Œç§»é™¤éèªéŸ³å…§å®¹

    Args:
        script_text: åŸå§‹è…³æœ¬æ–‡æœ¬

    Returns:
        æ¸…ç†å¾Œçš„ç´”è‹±æ–‡æ–‡æœ¬
    """
    # ç§»é™¤æš«åœæ¨™è¨˜ (pause)
    text = re.sub(r'\(pause\)', ' ', script_text)

    # ç§»é™¤ä¸­è‹±æ–‡å°ç…§è¨»è§£æˆ–å°æ¼”ç­†è¨˜ (æ‹¬è™Ÿå…§å®¹å«ä¸­æ–‡æˆ–å…¨éƒ¨å¤§å¯«)
    text = re.sub(r'\([^)]*[\u4e00-\u9fff][^)]*\)', ' ', text)
    text = re.sub(r'\([^)]*[A-Z]{3,}[^)]*\)', ' ', text)

    # ç§»é™¤æ–¹æ‹¬è™Ÿï¼èŠ±æ‹¬è™Ÿèªªæ˜
    text = re.sub(r'\[[^\]]*\]', ' ', text)
    text = re.sub(r'\{[^}]*\}', ' ', text)

    # å°‡ç ´æŠ˜è™Ÿæˆ–çœç•¥è™Ÿè½‰æ›ç‚ºå¥è™Ÿï¼Œä¾¿æ–¼åˆ†å¥
    text = text.replace('â€”', '.').replace('...', '. ')

    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
    text = re.sub(r'\s+', ' ', text)

    # ç§»é™¤é–‹é ­å’Œçµå°¾çš„ç©ºç™½
    text = text.strip()

    return text


def split_into_sentences(text: str) -> List[str]:
    """
    å°‡æ–‡æœ¬åˆ†å‰²æˆå¥å­

    Args:
        text: æ¸…ç†å¾Œçš„æ–‡æœ¬

    Returns:
        å¥å­åˆ—è¡¨
    """
    # æŒ‰å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿåˆ†å‰²
    sentences = re.split(r'(?<=[.!?])\s+', text)

    # éæ¿¾ç©ºå¥å­
    sentences = [s.strip() for s in sentences if s.strip()]

    return sentences


def prepare_segments(script_text: str, max_words_per_segment: int = 50) -> List[Dict[str, str]]:
    """
    æº–å‚™ WhisperX å°é½Šæ‰€éœ€çš„ segments æ ¼å¼

    Args:
        script_text: åŸå§‹è…³æœ¬æ–‡æœ¬
        max_words_per_segment: æ¯å€‹ segment çš„æœ€å¤§è©æ•¸

    Returns:
        segment åˆ—è¡¨ï¼Œæ ¼å¼: [{"text": "..."}, ...]
    """
    # æ¸…ç†æ–‡æœ¬
    clean_text = clean_script_for_alignment(script_text)

    # åˆ†å‰²æˆå¥å­
    sentences = split_into_sentences(clean_text)

    # çµ„åˆå¥å­æˆ segmentsï¼ˆé¿å…éé•·ï¼‰
    segments = []
    current_segment = []
    current_word_count = 0

    for sentence in sentences:
        words = sentence.split()
        sentence_word_count = len(words)

        # å¦‚æœåŠ å…¥é€™å¥æœƒè¶…éé™åˆ¶ï¼Œå…ˆå„²å­˜ç•¶å‰ segment
        if current_word_count + sentence_word_count > max_words_per_segment and current_segment:
            segments.append({"text": " ".join(current_segment)})
            current_segment = []
            current_word_count = 0

        # åŠ å…¥å¥å­
        current_segment.append(sentence)
        current_word_count += sentence_word_count

    # åŠ å…¥æœ€å¾Œä¸€å€‹ segment
    if current_segment:
        segments.append({"text": " ".join(current_segment)})

    return segments


def analyze_script(script_path: str) -> Dict[str, any]:
    """
    åˆ†æè…³æœ¬ï¼Œæä¾›æ¸…ç†å‰å¾Œçš„çµ±è¨ˆè³‡è¨Š

    Args:
        script_path: è…³æœ¬æ–‡ä»¶è·¯å¾‘

    Returns:
        çµ±è¨ˆè³‡è¨Šå­—å…¸
    """
    with open(script_path, 'r', encoding='utf-8') as f:
        original_text = f.read()

    clean_text = clean_script_for_alignment(original_text)
    segments = prepare_segments(original_text)

    return {
        "original_length": len(original_text),
        "original_word_count": len(original_text.split()),
        "clean_length": len(clean_text),
        "clean_word_count": len(clean_text.split()),
        "pause_marks_removed": original_text.count("(pause)"),
        "chinese_annotations_removed": len(re.findall(r'\([^)]*[\u4e00-\u9fff][^)]*\)', original_text)),
        "segment_count": len(segments),
        "segments": segments
    }


if __name__ == "__main__":
    import sys
    import json

    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹å¼: python text_cleaner.py <script_file>")
        sys.exit(1)

    script_file = sys.argv[1]

    print("=" * 60)
    print("ğŸ“ è…³æœ¬æ¸…ç†å·¥å…·")
    print("=" * 60)

    # åˆ†æè…³æœ¬
    stats = analyze_script(script_file)

    print(f"\nåŸå§‹çµ±è¨ˆï¼š")
    print(f"  å­—ç¬¦æ•¸: {stats['original_length']}")
    print(f"  è©æ•¸: {stats['original_word_count']}")

    print(f"\næ¸…ç†çµ±è¨ˆï¼š")
    print(f"  ç§»é™¤æš«åœæ¨™è¨˜: {stats['pause_marks_removed']} å€‹")
    print(f"  ç§»é™¤ä¸­æ–‡è¨»é‡‹: {stats['chinese_annotations_removed']} å€‹")
    print(f"  æ¸…ç†å¾Œå­—ç¬¦æ•¸: {stats['clean_length']}")
    print(f"  æ¸…ç†å¾Œè©æ•¸: {stats['clean_word_count']}")

    print(f"\nåˆ†æ®µè³‡è¨Šï¼š")
    print(f"  ç¸½æ®µè½æ•¸: {stats['segment_count']}")
    print(f"  å¹³å‡æ¯æ®µè©æ•¸: {stats['clean_word_count'] / stats['segment_count']:.1f}")

    # é¡¯ç¤ºå‰ 3 å€‹æ®µè½é è¦½
    print(f"\næ®µè½é è¦½ï¼ˆå‰ 3 å€‹ï¼‰ï¼š")
    for i, seg in enumerate(stats['segments'][:3], 1):
        preview = seg['text'][:80] + "..." if len(seg['text']) > 80 else seg['text']
        print(f"  {i}. {preview}")

    # è¼¸å‡º JSON æ ¼å¼çš„ segments
    output_file = script_file.replace('.txt', '_segments.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(stats['segments'], f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Segments å·²ä¿å­˜è‡³: {output_file}")
    print("=" * 60)
